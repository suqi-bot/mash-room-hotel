// ğŸ¤ å®æ—¶è¯­éŸ³é€šè¯æœåŠ¡
// æä¾›åŸºäºWebRTCçš„å®æ—¶è¯­éŸ³é€šè¯åŠŸèƒ½

export interface VoiceCallConfig {
  // éŸ³é¢‘é…ç½®
  sampleRate: number
  channels: number
  bitRate: number
  
  // VADé…ç½®
  vadEnabled: boolean
  vadThreshold: number
  vadDebounceTime: number
  
  // éŸ³é¢‘å¤„ç†é…ç½®
  echoCancellation: boolean
  noiseSuppression: boolean
  autoGainControl: boolean
  
  // AIé…ç½®
  aiResponseDelay: number
  maxSilenceTime: number
}

export interface VoiceCallState {
  isConnected: boolean
  isRecording: boolean
  isSpeaking: boolean
  isProcessing: boolean
  audioLevel: number
  networkQuality: 'excellent' | 'good' | 'fair' | 'poor'
  error?: string
}

export interface VoiceCallEvents {
  onStateChange: (state: VoiceCallState) => void
  onUserSpeechStart: () => void
  onUserSpeechEnd: (transcript: string) => void
  onAIResponseStart: () => void
  onAIResponseEnd: () => void
  onError: (error: Error) => void
  onAudioLevel: (level: number) => void
}

export class RealTimeVoiceService {
  private config: VoiceCallConfig
  private events: VoiceCallEvents
  private state: VoiceCallState
  
  // WebRTCç›¸å…³
  private mediaStream?: MediaStream
  private audioContext?: AudioContext
  private analyser?: AnalyserNode
  private processor?: ScriptProcessorNode
  
  // è¯­éŸ³è¯†åˆ«ç›¸å…³
  private recognition?: any // SpeechRecognitionç±»å‹åœ¨æŸäº›ç¯å¢ƒä¸‹ä¸å¯ç”¨
  private isListening = false
  
  // è¯­éŸ³åˆæˆç›¸å…³
  private speechSynthesis?: SpeechSynthesis
  private currentUtterance?: SpeechSynthesisUtterance
  
  // VADç›¸å…³
  private vadProcessor?: AudioWorkletNode
  private silenceTimer?: number
  private speechStartTime?: number
  
  // éŸ³é¢‘æ•°æ®ç¼“å†²
  private audioBuffer: Float32Array[] = []
  private isRecordingAudio = false

  constructor(config: Partial<VoiceCallConfig> = {}, events: Partial<VoiceCallEvents> = {}) {
    this.config = {
      sampleRate: 16000,
      channels: 1,
      bitRate: 64000,
      vadEnabled: true,
      vadThreshold: 0.01,
      vadDebounceTime: 300,
      echoCancellation: true,
      noiseSuppression: true,
      autoGainControl: true,
      aiResponseDelay: 500,
      maxSilenceTime: 3000,
      ...config
    }
    
    this.events = {
      onStateChange: () => {},
      onUserSpeechStart: () => {},
      onUserSpeechEnd: () => {},
      onAIResponseStart: () => {},
      onAIResponseEnd: () => {},
      onError: () => {},
      onAudioLevel: () => {},
      ...events
    }
    
    this.state = {
      isConnected: false,
      isRecording: false,
      isSpeaking: false,
      isProcessing: false,
      audioLevel: 0,
      networkQuality: 'good'
    }
    
    this.initializeServices()
  }

  /**
   * åˆå§‹åŒ–è¯­éŸ³æœåŠ¡
   */
  private async initializeServices() {
    try {
      // åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«
      this.initializeSpeechRecognition()
      
      // åˆå§‹åŒ–è¯­éŸ³åˆæˆ
      this.initializeSpeechSynthesis()
      
      // åˆå§‹åŒ–éŸ³é¢‘ä¸Šä¸‹æ–‡
      await this.initializeAudioContext()
      
      console.log('å®æ—¶è¯­éŸ³æœåŠ¡åˆå§‹åŒ–å®Œæˆ')
    } catch (error) {
      console.error('è¯­éŸ³æœåŠ¡åˆå§‹åŒ–å¤±è´¥:', error)
      this.handleError(error as Error)
    }
  }

  /**
   * åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«
   */
  private initializeSpeechRecognition() {
    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
      const SpeechRecognition = (window as any).webkitSpeechRecognition || (window as any).SpeechRecognition
      this.recognition = new SpeechRecognition()
      
      this.recognition.continuous = true
      this.recognition.interimResults = true
      this.recognition.lang = 'zh-CN'
      
      this.recognition.onstart = () => {
        this.isListening = true
        this.updateState({ isRecording: true })
        this.events.onUserSpeechStart()
      }
      
      this.recognition.onresult = (event: any) => {
        let finalTranscript = ''
        let interimTranscript = ''
        
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript
          if (event.results[i].isFinal) {
            finalTranscript += transcript
          } else {
            interimTranscript += transcript
          }
        }
        
        if (finalTranscript) {
          this.handleUserSpeechEnd(finalTranscript)
        }
      }
      
      this.recognition.onerror = (event: any) => {
        console.error('è¯­éŸ³è¯†åˆ«é”™è¯¯:', event.error)
        this.handleError(new Error(`è¯­éŸ³è¯†åˆ«é”™è¯¯: ${event.error}`))
      }
      
      this.recognition.onend = () => {
        this.isListening = false
        if (this.state.isRecording && !this.state.isProcessing) {
          // å¦‚æœè¿˜åœ¨é€šè¯ä¸­ï¼Œé‡æ–°å¼€å§‹è¯†åˆ«
          setTimeout(() => {
            if (this.state.isConnected && !this.state.isSpeaking) {
              this.startListening()
            }
          }, 100)
        }
      }
    } else {
      throw new Error('æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½')
    }
  }

  /**
   * åˆå§‹åŒ–è¯­éŸ³åˆæˆ
   */
  private initializeSpeechSynthesis() {
    if ('speechSynthesis' in window) {
      this.speechSynthesis = window.speechSynthesis
    } else {
      throw new Error('æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³åˆæˆåŠŸèƒ½')
    }
  }

  /**
   * åˆå§‹åŒ–éŸ³é¢‘ä¸Šä¸‹æ–‡
   */
  private async initializeAudioContext() {
    try {
      this.audioContext = new (window.AudioContext || (window as any).webkitAudioContext)()
      
      if (this.audioContext.state === 'suspended') {
        await this.audioContext.resume()
      }
      
      // åˆ›å»ºåˆ†æå™¨èŠ‚ç‚¹ç”¨äºéŸ³é‡æ£€æµ‹
      this.analyser = this.audioContext.createAnalyser()
      this.analyser.fftSize = 256
      this.analyser.smoothingTimeConstant = 0.8
      
    } catch (error) {
      throw new Error(`éŸ³é¢‘ä¸Šä¸‹æ–‡åˆå§‹åŒ–å¤±è´¥: ${error}`)
    }
  }

  /**
   * å¼€å§‹è¯­éŸ³é€šè¯
   */
  async startVoiceCall(): Promise<void> {
    try {
      // è¯·æ±‚éº¦å…‹é£æƒé™
      await this.requestMicrophoneAccess()
      
      // å¼€å§‹éŸ³é¢‘ç›‘å¬
      this.startAudioMonitoring()
      
      // å¼€å§‹è¯­éŸ³è¯†åˆ«
      this.startListening()
      
      this.updateState({ isConnected: true })
      
      console.log('è¯­éŸ³é€šè¯å·²å¼€å§‹')
    } catch (error) {
      console.error('å¯åŠ¨è¯­éŸ³é€šè¯å¤±è´¥:', error)
      this.handleError(error as Error)
    }
  }

  /**
   * ç»“æŸè¯­éŸ³é€šè¯
   */
  async endVoiceCall(): Promise<void> {
    try {
      // åœæ­¢è¯­éŸ³è¯†åˆ«
      this.stopListening()
      
      // åœæ­¢è¯­éŸ³åˆæˆ
      this.stopSpeaking()
      
      // åœæ­¢éŸ³é¢‘ç›‘å¬
      this.stopAudioMonitoring()
      
      // é‡Šæ”¾åª’ä½“æµ
      if (this.mediaStream) {
        this.mediaStream.getTracks().forEach(track => track.stop())
        this.mediaStream = undefined
      }
      
      this.updateState({ 
        isConnected: false, 
        isRecording: false, 
        isSpeaking: false, 
        isProcessing: false 
      })
      
      console.log('è¯­éŸ³é€šè¯å·²ç»“æŸ')
    } catch (error) {
      console.error('ç»“æŸè¯­éŸ³é€šè¯å¤±è´¥:', error)
      this.handleError(error as Error)
    }
  }

  /**
   * è¯·æ±‚éº¦å…‹é£è®¿é—®æƒé™
   */
  private async requestMicrophoneAccess(): Promise<void> {
    try {
      const constraints = {
        audio: {
          echoCancellation: this.config.echoCancellation,
          noiseSuppression: this.config.noiseSuppression,
          autoGainControl: this.config.autoGainControl,
          sampleRate: this.config.sampleRate,
          channelCount: this.config.channels
        }
      }
      
      this.mediaStream = await navigator.mediaDevices.getUserMedia(constraints)
      console.log('éº¦å…‹é£è®¿é—®æƒé™å·²è·å–')
    } catch (error) {
      throw new Error(`æ— æ³•è®¿é—®éº¦å…‹é£: ${error}`)
    }
  }

  /**
   * å¼€å§‹éŸ³é¢‘ç›‘å¬
   */
  private startAudioMonitoring(): void {
    if (!this.audioContext || !this.analyser || !this.mediaStream) return
    
    const source = this.audioContext.createMediaStreamSource(this.mediaStream)
    source.connect(this.analyser)
    
    // å¼€å§‹éŸ³é‡æ£€æµ‹
    this.startVolumeDetection()
  }

  /**
   * åœæ­¢éŸ³é¢‘ç›‘å¬
   */
  private stopAudioMonitoring(): void {
    if (this.processor) {
      this.processor.disconnect()
      this.processor = undefined
    }
  }

  /**
   * å¼€å§‹éŸ³é‡æ£€æµ‹
   */
  private startVolumeDetection(): void {
    if (!this.analyser) return
    
    const bufferLength = this.analyser.frequencyBinCount
    const dataArray = new Uint8Array(bufferLength)
    
    const detectVolume = () => {
      if (!this.state.isConnected) return
      
      this.analyser!.getByteFrequencyData(dataArray)
      
      // è®¡ç®—å¹³å‡éŸ³é‡
      let sum = 0
      for (let i = 0; i < bufferLength; i++) {
        sum += dataArray[i]
      }
      const average = sum / bufferLength
      const normalizedLevel = average / 255
      
      this.updateState({ audioLevel: normalizedLevel })
      this.events.onAudioLevel(normalizedLevel)
      
      requestAnimationFrame(detectVolume)
    }
    
    detectVolume()
  }

  /**
   * AIè¯­éŸ³å›å¤
   */
  async speakAIResponse(text: string): Promise<void> {
    if (!this.speechSynthesis || !text.trim()) return

    try {
      this.updateState({ isSpeaking: true, isProcessing: true })
      this.events.onAIResponseStart()

      // æ¸…ç†æ–‡æœ¬
      const cleanText = this.cleanTextForSpeech(text)

      await this.synthesizeSpeech(cleanText)

      this.updateState({ isSpeaking: false, isProcessing: false })
      this.events.onAIResponseEnd()

      // AIè¯´å®Œåï¼Œé‡æ–°å¼€å§‹ç›‘å¬ç”¨æˆ·
      if (this.state.isConnected) {
        setTimeout(() => {
          this.startListening()
        }, this.config.aiResponseDelay)
      }

    } catch (error) {
      console.error('AIè¯­éŸ³å›å¤å¤±è´¥:', error)
      this.handleError(error as Error)
    }
  }

  /**
   * è¯­éŸ³åˆæˆ
   */
  private synthesizeSpeech(text: string): Promise<void> {
    return new Promise((resolve, reject) => {
      if (!this.speechSynthesis) {
        reject(new Error('è¯­éŸ³åˆæˆä¸å¯ç”¨'))
        return
      }

      // åœæ­¢å½“å‰æ’­æ”¾
      this.speechSynthesis.cancel()

      const utterance = new SpeechSynthesisUtterance(text)

      // é…ç½®è¯­éŸ³å‚æ•°ï¼ˆå°‘å¹´éŸ³è‰²ï¼‰
      utterance.lang = 'zh-CN'
      utterance.rate = 1.1
      utterance.pitch = 1.3
      utterance.volume = 0.8

      // å°è¯•ä½¿ç”¨æ›´å¥½çš„ä¸­æ–‡è¯­éŸ³
      const voices = this.speechSynthesis.getVoices()
      const chineseVoice = voices.find(voice =>
        voice.lang.includes('zh') && voice.name.includes('Xiaoxiao')
      ) || voices.find(voice => voice.lang.includes('zh'))

      if (chineseVoice) {
        utterance.voice = chineseVoice
      }

      utterance.onstart = () => {
        console.log('AIå¼€å§‹è¯´è¯')
      }

      utterance.onend = () => {
        resolve()
      }

      utterance.onerror = (event) => {
        reject(new Error(`è¯­éŸ³åˆæˆé”™è¯¯: ${event.error}`))
      }

      this.currentUtterance = utterance
      this.speechSynthesis.speak(utterance)
    })
  }

  /**
   * æ¸…ç†æ–‡æœ¬ç”¨äºè¯­éŸ³åˆæˆ
   */
  private cleanTextForSpeech(text: string): string {
    return text
      .replace(/ğŸ„|ğŸ§­|ğŸ¡|ğŸ“…|ğŸ½ï¸|â˜•|ğŸŒ¿|ğŸ¤|ğŸ—ºï¸|ğŸ’°|â­|ğŸ“|ğŸ˜Š|ğŸ˜„|ğŸ¤”/g, '') // ç§»é™¤emoji
      .replace(/\*\*(.*?)\*\*/g, '$1') // ç§»é™¤markdownç²—ä½“
      .replace(/\n/g, ' ') // æ¢è¡Œæ›¿æ¢ä¸ºç©ºæ ¼
      .replace(/\s+/g, ' ') // å¤šä¸ªç©ºæ ¼åˆå¹¶
      .trim()
  }

  /**
   * æš‚åœè¯­éŸ³é€šè¯
   */
  pauseVoiceCall(): void {
    this.stopListening()
    this.stopSpeaking()
    this.updateState({ isRecording: false, isSpeaking: false })
  }

  /**
   * æ¢å¤è¯­éŸ³é€šè¯
   */
  resumeVoiceCall(): void {
    if (this.state.isConnected && !this.state.isSpeaking) {
      this.startListening()
    }
  }

  /**
   * è·å–å½“å‰çŠ¶æ€
   */
  getState(): VoiceCallState {
    return { ...this.state }
  }

  /**
   * æ£€æŸ¥æµè§ˆå™¨æ”¯æŒ
   */
  static checkBrowserSupport(): {
    speechRecognition: boolean
    speechSynthesis: boolean
    mediaDevices: boolean
    audioContext: boolean
  } {
    return {
      speechRecognition: 'webkitSpeechRecognition' in window || 'SpeechRecognition' in window,
      speechSynthesis: 'speechSynthesis' in window,
      mediaDevices: 'mediaDevices' in navigator && 'getUserMedia' in navigator.mediaDevices,
      audioContext: 'AudioContext' in window || 'webkitAudioContext' in window
    }
  }

  /**
   * è®¾ç½®éŸ³é‡é˜ˆå€¼
   */
  setVolumeThreshold(threshold: number): void {
    this.config.vadThreshold = Math.max(0, Math.min(1, threshold))
  }

  /**
   * è·å–éŸ³é¢‘è®¾å¤‡åˆ—è¡¨
   */
  async getAudioDevices(): Promise<MediaDeviceInfo[]> {
    try {
      const devices = await navigator.mediaDevices.enumerateDevices()
      return devices.filter(device => device.kind === 'audioinput')
    } catch (error) {
      console.error('è·å–éŸ³é¢‘è®¾å¤‡å¤±è´¥:', error)
      return []
    }
  }

  /**
   * åˆ‡æ¢éŸ³é¢‘è®¾å¤‡
   */
  async switchAudioDevice(deviceId: string): Promise<void> {
    try {
      // åœæ­¢å½“å‰æµ
      if (this.mediaStream) {
        this.mediaStream.getTracks().forEach(track => track.stop())
      }

      // ä½¿ç”¨æ–°è®¾å¤‡åˆ›å»ºæµ
      const constraints = {
        audio: {
          deviceId: { exact: deviceId },
          echoCancellation: this.config.echoCancellation,
          noiseSuppression: this.config.noiseSuppression,
          autoGainControl: this.config.autoGainControl
        }
      }

      this.mediaStream = await navigator.mediaDevices.getUserMedia(constraints)

      // é‡æ–°è¿æ¥éŸ³é¢‘ç›‘å¬
      if (this.state.isConnected) {
        this.startAudioMonitoring()
      }

      console.log('éŸ³é¢‘è®¾å¤‡åˆ‡æ¢æˆåŠŸ')
    } catch (error) {
      console.error('åˆ‡æ¢éŸ³é¢‘è®¾å¤‡å¤±è´¥:', error)
      this.handleError(error as Error)
    }
  }

  private updateState(newState: Partial<VoiceCallState>): void {
    this.state = { ...this.state, ...newState }
    this.events.onStateChange(this.state)
  }

  private handleError(error: Error): void {
    this.updateState({ error: error.message })
    this.events.onError(error)
  }

  private handleUserSpeechEnd(transcript: string): void {
    this.updateState({ isProcessing: true })
    this.events.onUserSpeechEnd(transcript)
  }

  private startListening(): void {
    if (this.recognition && !this.isListening && !this.state.isSpeaking) {
      try {
        this.recognition.start()
      } catch (error) {
        console.error('å¯åŠ¨è¯­éŸ³è¯†åˆ«å¤±è´¥:', error)
      }
    }
  }

  private stopListening(): void {
    if (this.recognition && this.isListening) {
      this.recognition.stop()
    }
  }

  private stopSpeaking(): void {
    if (this.speechSynthesis) {
      this.speechSynthesis.cancel()
    }
    if (this.currentUtterance) {
      this.currentUtterance = undefined
    }
  }
}
